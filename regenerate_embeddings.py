# regenerate_embeddings.py
import os
import pandas as pd
import numpy as np
import requests
import time
from config import Config, SiliconFlowEmbeddings


def regenerate_book_embeddings():
    """é‡æ–°ç”Ÿæˆä¹¦ç±åµŒå…¥å‘é‡"""
    print("ğŸ”„ å¼€å§‹é‡æ–°ç”Ÿæˆä¹¦ç±åµŒå…¥å‘é‡...")

    # è¯»å–åŸå§‹æ•°æ®
    try:
        df = pd.read_csv("book_embeddings.csv", encoding="utf-8-sig")
        print(f"ğŸ“– è¯»å–åˆ° {len(df)} æ¡åŸå§‹æ•°æ®")
    except Exception as e:
        print(f"âŒ è¯»å–æ•°æ®å¤±è´¥: {e}")
        return

    # åˆå§‹åŒ–åµŒå…¥ç”Ÿæˆå™¨
    embeddings = SiliconFlowEmbeddings()

    # åˆ†æ‰¹é‡æ–°ç”ŸæˆåµŒå…¥å‘é‡
    BATCH_SIZE = 20  # å‡å°æ‰¹æ¬¡å¤§å°é¿å…APIé™åˆ¶
    total_batches = (len(df) + BATCH_SIZE - 1) // BATCH_SIZE

    new_embeddings = []
    success_count = 0
    failed_count = 0

    print(f"ğŸ”„ åˆ† {total_batches} æ‰¹é‡æ–°ç”ŸæˆåµŒå…¥å‘é‡...")

    for batch_idx in range(total_batches):
        start_idx = batch_idx * BATCH_SIZE
        end_idx = min((batch_idx + 1) * BATCH_SIZE, len(df))

        batch_texts = []
        batch_indices = []

        # å‡†å¤‡æ‰¹æ¬¡æ•°æ®
        for i in range(start_idx, end_idx):
            try:
                text = str(df.iloc[i]["text"]).strip()
                if text and len(text) > 10:  # ç¡®ä¿æ–‡æœ¬æœ‰æ•ˆ
                    batch_texts.append(text)
                    batch_indices.append(i)
            except:
                continue

        if not batch_texts:
            continue

        print(f"å¤„ç†æ‰¹æ¬¡ {batch_idx + 1}/{total_batches} ({len(batch_texts)}æ¡æ–‡æœ¬)...")

        # ç”ŸæˆåµŒå…¥å‘é‡
        try:
            batch_embeddings = embeddings.embed_documents(batch_texts)

            for idx, embedding in zip(batch_indices, batch_embeddings):
                if len(embedding) == 1024:
                    # å°†åµŒå…¥å‘é‡è½¬æ¢ä¸ºå­—ç¬¦ä¸²å­˜å‚¨
                    embedding_str = ",".join(map(str, embedding))
                    new_embeddings.append((idx, embedding_str))
                    success_count += 1
                else:
                    failed_count += 1

            print(f"âœ… æ‰¹æ¬¡ {batch_idx + 1} å®Œæˆ")

        except Exception as e:
            print(f"âŒ æ‰¹æ¬¡ {batch_idx + 1} å¤±è´¥: {e}")
            failed_count += len(batch_texts)

        # é¿å…APIé™åˆ¶
        time.sleep(2)

    print(f"\nğŸ“Š é‡æ–°ç”Ÿæˆå®Œæˆ:")
    print(f"   - æˆåŠŸ: {success_count}æ¡")
    print(f"   - å¤±è´¥: {failed_count}æ¡")

    # æ›´æ–°DataFrame
    if success_count > 0:
        # åˆ›å»ºæ–°çš„åµŒå…¥å‘é‡åˆ—
        embedding_dict = {idx: emb for idx, emb in new_embeddings}
        df['new_embedding'] = df.index.map(embedding_dict)

        # ä¿å­˜æ–°çš„æ•°æ®æ–‡ä»¶
        output_file = "book_embeddings_renewed.csv"
        df.to_csv(output_file, index=False, encoding="utf-8-sig")
        print(f"ğŸ’¾ æ–°çš„åµŒå…¥æ–‡ä»¶å·²ä¿å­˜: {output_file}")

        return output_file
    else:
        print("âŒ æ²¡æœ‰æˆåŠŸç”Ÿæˆä»»ä½•åµŒå…¥å‘é‡")
        return None


def test_new_embeddings(file_path):
    """æµ‹è¯•æ–°ç”Ÿæˆçš„åµŒå…¥å‘é‡"""
    print(f"\nğŸ§ª æµ‹è¯•æ–°åµŒå…¥å‘é‡æ–‡ä»¶: {file_path}")

    try:
        df = pd.read_csv(file_path, encoding="utf-8-sig")
        print(f"ğŸ“Š æ•°æ®ç»Ÿè®¡: {len(df)} æ¡è®°å½•")

        # æ£€æŸ¥åµŒå…¥å‘é‡è´¨é‡
        if 'new_embedding' in df.columns:
            sample_embedding = df.iloc[0]['new_embedding']
            if pd.notna(sample_embedding):
                vector = list(map(float, str(sample_embedding).split(",")))
                print(f"ğŸ” æ–°åµŒå…¥å‘é‡æ£€æŸ¥:")
                print(f"   - ç»´åº¦: {len(vector)}")
                print(f"   - èŒƒå›´: [{min(vector):.4f}, {max(vector):.4f}]")
                print(f"   - å‡å€¼: {np.mean(vector):.4f}")

        # æ£€æŸ¥ä½œè€…åˆ†å¸ƒ
        if 'author' in df.columns:
            author_counts = df['author'].value_counts()
            print(f"ğŸ“š ä½œè€…åˆ†å¸ƒ (å‰5):")
            for author, count in author_counts.head(5).items():
                print(f"   - {author}: {count}æ¡")

    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")


def create_faiss_with_new_embeddings(file_path):
    """ä½¿ç”¨æ–°åµŒå…¥å‘é‡åˆ›å»ºFAISSç´¢å¼•"""
    print(f"\nğŸ”§ ä½¿ç”¨æ–°åµŒå…¥å‘é‡åˆ›å»ºFAISSç´¢å¼•...")

    try:
        df = pd.read_csv(file_path, encoding="utf-8-sig")

        # å‡†å¤‡æ•°æ®
        texts = []
        metadatas = []
        embeddings_list = []

        success_count = 0
        for _, row in df.iterrows():
            try:
                # ä½¿ç”¨æ–°çš„åµŒå…¥å‘é‡
                if 'new_embedding' not in row or pd.isna(row['new_embedding']):
                    continue

                embedding_str = str(row['new_embedding']).strip()
                embedding = list(map(float, embedding_str.split(",")))
                if len(embedding) != 1024:
                    continue

                text = str(row["text"]) if "text" in row else ""
                if not text.strip():
                    continue

                texts.append(text)
                metadatas.append({
                    "title": str(row.get("title", "æ— é¢˜å")),
                    "author": str(row.get("author", "æœªçŸ¥ä½œè€…")),
                    "publisher": str(row.get("publisher", "æœªçŸ¥å‡ºç‰ˆç¤¾")),
                    "year": str(row.get("year", "æœªçŸ¥å¹´ä»½")),
                    "chunk_id": str(row.get("chunk_id", "")),
                    "book_id": str(row.get("book_id", ""))
                })
                embeddings_list.append(embedding)
                success_count += 1

            except Exception as e:
                continue

        print(f"âœ… å‡†å¤‡ {success_count} æ¡æœ‰æ•ˆè®°å½•")

        if success_count == 0:
            raise Exception("æ²¡æœ‰æœ‰æ•ˆçš„è®°å½•")

        # åˆ›å»ºFAISSç´¢å¼•
        embeddings = SiliconFlowEmbeddings()
        from langchain_community.vectorstores import FAISS
        import numpy as np

        vectorstore = FAISS.from_embeddings(
            text_embeddings=list(zip(texts, embeddings_list)),
            embedding=embeddings,
            metadatas=metadatas
        )

        # ä¿å­˜ç´¢å¼•
        new_index_path = "./faiss_renewed_index"
        vectorstore.save_local(new_index_path)
        print(f"ğŸ’¾ æ–°FAISSç´¢å¼•å·²ä¿å­˜åˆ°: {new_index_path}")

        return new_index_path, vectorstore

    except Exception as e:
        print(f"âŒ åˆ›å»ºFAISSç´¢å¼•å¤±è´¥: {e}")
        return None, None


def test_search_accuracy(vectorstore):
    """æµ‹è¯•æœç´¢å‡†ç¡®æ€§"""
    print(f"\nğŸ¯ æµ‹è¯•æœç´¢å‡†ç¡®æ€§...")

    test_queries = [
        "å·´é‡‘",
        "é²è¿…",
        "å°è¯´",
        "å†å²",
        "è€èˆ",
        "éƒ­æ²«è‹¥"
    ]

    for query in test_queries:
        print(f"\nğŸ” æœç´¢: '{query}'")
        try:
            docs = vectorstore.similarity_search(query, k=3)

            for i, doc in enumerate(docs):
                title = doc.metadata.get('title', 'æ— é¢˜å')
                author = doc.metadata.get('author', 'æœªçŸ¥ä½œè€…')
                print(f"  {i + 1}. ã€Š{title}ã€‹ - {author}")

                # æ£€æŸ¥æ˜¯å¦ç›¸å…³
                if query in author or query in title:
                    print(f"     âœ… ç›¸å…³!")
                else:
                    print(f"     âŒ ä¸ç›¸å…³")

        except Exception as e:
            print(f"  âŒ æœç´¢å¤±è´¥: {e}")


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸ”„ ä¹¦ç±åµŒå…¥å‘é‡é‡æ–°ç”Ÿæˆå·¥å…·")
    print("=" * 60)

    # æ­¥éª¤1: é‡æ–°ç”ŸæˆåµŒå…¥å‘é‡
    print("\n1. é‡æ–°ç”ŸæˆåµŒå…¥å‘é‡")
    new_file = regenerate_book_embeddings()

    if not new_file:
        print("âŒ é‡æ–°ç”Ÿæˆå¤±è´¥ï¼Œé€€å‡ºç¨‹åº")
        return

    # æ­¥éª¤2: æµ‹è¯•æ–°åµŒå…¥å‘é‡
    print("\n2. æµ‹è¯•æ–°åµŒå…¥å‘é‡")
    test_new_embeddings(new_file)

    # æ­¥éª¤3: åˆ›å»ºæ–°çš„FAISSç´¢å¼•
    print("\n3. åˆ›å»ºæ–°çš„FAISSç´¢å¼•")
    new_index_path, vectorstore = create_faiss_with_new_embeddings(new_file)

    if vectorstore:
        # æ­¥éª¤4: æµ‹è¯•æœç´¢å‡†ç¡®æ€§
        print("\n4. æµ‹è¯•æœç´¢å‡†ç¡®æ€§")
        test_search_accuracy(vectorstore)

        print(f"\nğŸ‰ é‡æ–°ç”Ÿæˆå®Œæˆ!")
        print(f"   æ–°æ•°æ®æ–‡ä»¶: {new_file}")
        print(f"   æ–°ç´¢å¼•è·¯å¾„: {new_index_path}")
        print(f"\nğŸ’¡ è¯·æ›´æ–° config.py ä¸­çš„è·¯å¾„é…ç½®:")
        print(f"   FAISS_INDEX_PATH = '{new_index_path}'")
        print(f"   BOOKS_DATA_PATH = '{new_file}'")


if __name__ == "__main__":
    main()